{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from experiments.helpers import plot_all, print_metrics, plot_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./dataset/house_price_train.csv', parse_dates=[\"Time\"])\n",
    "df_test = pd.read_csv('./dataset/house_price_test.csv', parse_dates=[\"Time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\"City\", \"District\", \"Street\", \"Community\", \"Floor\"] # [\"City\", \"Floor\"]\n",
    "num_columns = [\"#Floors\", \"#Rooms\", \"#Halls\", \"Area\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unused columns\n",
    "df_train_pool = df_train.drop(columns=[\"Id\", \"Time\", \"Orient\", \"Lat\", \"Lon\"])\n",
    "df_test_pool = df_train.drop(columns=[\"Id\", \"Time\", \"Orient\", \"Lat\", \"Lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize price\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train_pool[\"Price\"].values.reshape(-1, 1))\n",
    "\n",
    "y_train = scaler.transform(df_train_pool[\"Price\"].values.reshape(-1, 1))\n",
    "y_test = scaler.transform(df_test_pool[\"Price\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = df_train_pool[num_columns].values\n",
    "X_test_num = df_test_pool[num_columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning based model using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_inputs = []\n",
    "models_intermediate = []\n",
    "\n",
    "# create categorical embedding model\n",
    "categorical_models = []\n",
    "for col in cat_columns:\n",
    "    vocab = list(df_train_pool[col].unique())\n",
    "    vocab_size = np.minimum(1000, df_train_pool[col].nunique() + 1)\n",
    "    print(f\"{col} vocab size: {vocab_size}\")\n",
    "\n",
    "    cat_input = tf.keras.layers.Input(shape=(1,), name=f\"cat_{col}\", dtype=tf.string)\n",
    "    cat_lookup = tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=None, num_oov_indices=0)(cat_input)\n",
    "    cat_embedding = tf.keras.layers.Embedding(vocab_size, 200)(cat_lookup)\n",
    "    cat_flatten = tf.keras.layers.Flatten()(cat_embedding)\n",
    "    \n",
    "    model_inputs.append(cat_input)\n",
    "    categorical_models.append(cat_flatten)\n",
    "\n",
    "# create numerical input\n",
    "num_normalizer = tf.keras.layers.Normalization()\n",
    "num_normalizer.adapt(X_train_num)\n",
    "\n",
    "numerical_input = tf.keras.layers.Input(shape=(4,), name=\"num_input\")\n",
    "numerical_normalize_layer = num_normalizer(numerical_input)\n",
    "\n",
    "model_inputs.append(numerical_input)\n",
    "\n",
    "# merge all inputs\n",
    "cat_num_concat = tf.keras.layers.concatenate([*categorical_models, numerical_normalize_layer])\n",
    "\n",
    "# reshape to 3D for RNN\n",
    "intermediate_reshape_layer = tf.keras.layers.Reshape(target_shape=(1, 4 + len(categorical_models) * 200))(cat_num_concat)\n",
    "\n",
    "# create LSTM layer\n",
    "lstm1_layer = tf.keras.layers.LSTM(64, return_sequences=True)(intermediate_reshape_layer)\n",
    "lstm2_layer = tf.keras.layers.LSTM(64)(lstm1_layer)\n",
    "\n",
    "# create output layer\n",
    "output_layer = tf.keras.layers.Dense(1)(lstm2_layer)\n",
    "\n",
    "# create final model\n",
    "baseline_model = tf.keras.Model(inputs=model_inputs, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(baseline_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/baseline_lstm_akhir\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "train_inputs = {\"num_input\": X_train_num}\n",
    "for col in cat_columns:\n",
    "    train_inputs[f\"cat_{col}\"] = df_train_pool[col].values.reshape(-1, 1)\n",
    "\n",
    "history = baseline_model.fit(train_inputs, y_train, validation_split=0.15, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = {\"num_input\": X_test_num}\n",
    "for col in cat_columns:\n",
    "    test_inputs[f\"cat_{col}\"] = df_test_pool[col].values.reshape(-1, 1)\n",
    "\n",
    "predicted = baseline_model.predict(test_inputs)\n",
    "predicted_unscaled = scaler.inverse_transform(predicted.reshape(-1, 1))\n",
    "y_true_unscaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "print_metrics(y_true_unscaled, predicted_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(y_true_unscaled, predicted_unscaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ieeecyberc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14a2fe69f7218c389a62750d90cfb6d5c94e95bd8d6c3ff1ef828a24358d27f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
